from bs4 import BeautifulSoup
import pandas as pd
import re
import time
from typing import Optional, Dict
from src.exceptions import ParseError, SaveToExcelError
import logging

logger = logging.getLogger(__name__)

def cvss_edited(cvss):
    try:
        # Удаляем все, кроме цифр и запятой
        number = re.sub(r'[^\d,]', '', cvss)
        # Заменяем запятую на точку, если она есть
        number = number.replace(',', '.')
        value = float(number)
        if value < 4:
            return f'{value} Low'
        elif value < 7:
            return f'{value} Medium'
        elif value < 9:
            return f'{value} High'
        else:
            return f'{value} Critical'
    except Exception:
        return 'N/A'

def find_main_table_with_retry(soup, max_attempts=2, delay=5):
    '''Поиск основной таблицы с повторной попыткой'''
    for attempt in range(max_attempts):
        main_table = soup.find('table', class_='table')
        if main_table:
            return main_table
        
        if attempt < max_attempts - 1:
            logger.debug(f'Table not found, retrying in {delay} seconds... (attempt {attempt + 1})')
            time.sleep(delay)
    
    logger.warning('Main table not found after')
    return None

class VulnerabilityParser:
    def parse_vulnerability_data(self, html: str, url: str) -> pd.DataFrame:
        try:
            soup = BeautifulSoup(html, 'html.parser')
            data_list = self._extract_data(soup, url)
            if data_list and isinstance(data_list[0], dict) and 'should_stop' in data_list[0]:
                return pd.DataFrame(data_list)
            return pd.DataFrame(data_list)
        except Exception as e:
            logger.error(f'Parsing failed: {str(e)}')
            return pd.DataFrame([{'should_stop': True}])

    def _extract_data(self, soup: BeautifulSoup, url: str) -> Optional[Dict[str, str]]:
        data_list = []
        main_table = find_main_table_with_retry(soup)
        if not main_table:
            return [{'should_skip': True}]

        product = []
        for row in main_table.find_all('tr')[1:]:  # Пропускаем заголовок
            cells = row.find_all('td')
            if len(cells) < 4:
                continue
            vendor = cells[0].find('span').text.strip() if cells[0].find('span') else 'N/A'
            product_str = cells[1].find('span').text.strip() if cells[1].find('span') else 'N/A'
            version = cells[2].text.strip()
            if version != '-':
                product.append(f'{product_str} {version}')
            else:
                product.append(product_str)

            type_ = cells[3].text.strip()
            type_ = re.sub(r'([а-я])([А-Я])', r'\1 \2', type_)

        product_all = ', '.join(product[1:])

        if vendor != 'N/A' and product != 'N/A':  # Фильтруем строки с 'N/A'
            data_list.append({
                'Вендор': vendor,
                'Продукт': product_all,
                'Тип': type_,
            })

        # Парсим CVSS и CVE
        cvss = 'N/A'
        cve = 'N/A'
        for row in main_table.find_all('tr'):
            cells = row.find_all('td')
            if len(cells) < 2:
                continue
            header = cells[0].get_text(strip=True).lower()
            value = cells[1].get_text(strip=True)
            if 'уровень опасности уязвимости' in header:
                # Обновленное регулярное выражение для CVSS 3.x
                cvss_match = re.search(r'CVSS 3\.\d+ составляет (\d+(,\d+)?)', value)
                if cvss_match:
                    cvss_value = cvss_match.group(1)
                    cvss = cvss_edited(cvss_value)
            elif 'идентификаторы других систем описаний уязвимостей' in header:
                cve_matches = re.findall(r'CVE-\d{4}-\d{4,}', value)
                if cve_matches:
                    cve = ', '.join(cve_matches)

        for data in data_list:
            data['CVSS'] = cvss
            data['CVE'] = cve
            data['URL'] = url

        if not data_list:
            return [{'should_stop': True}]
        return data_list

    def save_to_excel(self, df: pd.DataFrame, output_file: str) -> None:
        try:
            if not df.empty:
                df.to_excel(output_file, index=False)
        except Exception as e:
            raise SaveToExcelError(f'Failed to save to Excel: {str(e)}')
